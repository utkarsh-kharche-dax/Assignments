{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Donation_np\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file into dataframe\n",
    "df = spark.read.option('multiline', True).json(\"donation_np.json\").limit(100)\n",
    "\n",
    "dropcols = {'field10', 'field11', 'field12', 'field13', 'field14', 'field9'}\n",
    "df = df.drop(*dropcols)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "df=df.withColumn(\"Amount\",df[\"Amount\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tChange below column names:\n",
    "\n",
    "# Contribution Mode\tmode_of_payment\n",
    "# Financial Year\tfin_year\n",
    "# PAN Given\tpan_given\n",
    "\n",
    "df = df.withColumnRenamed(\"Contribution Mode\",\"mode_of_payment\") \\\n",
    "    .withColumnRenamed(\"Financial Year\",\"fin_year\") \\\n",
    "    .withColumnRenamed(\"PAN Given\",\"pan_given\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tEncrypt address column\n",
    "\n",
    "# 16-B, Ferozeshah Road New Delhi-1\taebd8d41127096039df138069fab7630\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import sha2,concat_ws\n",
    "df=df.withColumn(\"Address\", sha2(concat_ws(\"||\", df.Address), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tCategorize mode_of_payment into below 4 categories\n",
    "# o\tCash\n",
    "# o\tCheque\n",
    "# o\tBank\n",
    "# o\tOthers\n",
    "df = df.withColumn(\"mode_of_payment\",when(lower(df.mode_of_payment).rlike(\"\\d{5,6} | ch no | cheque no | cheque | ch. no. | ch.no.\"),\"Cheque\") \\\n",
    "      .when(lower(df.mode_of_payment).contains(\"cash\"),\"Cash\") \\\n",
    "      .when(lower(df.mode_of_payment).contains(\"bank\"),\"Bank\") \\\n",
    "      .otherwise(\"Others\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tCalculate aggregates per party\n",
    "# (below columns should b)\n",
    "\n",
    "# 'INC_SUM_LTD',\n",
    "#  'BJP_SUM_LTD',\n",
    "#  'NCP_SUM_LTD',\n",
    "#  'CPI_SUM_LTD',\n",
    "#  'CPI(M)_SUM_LTD',\n",
    "#  'INC_COUNT_LTD',\n",
    "#  'BJP_COUNT_LTD',\n",
    "#  'NCP_COUNT_LTD',\n",
    "#  'CPI_COUNT_LTD',\n",
    "#  'CPI(M)_COUNT_LTD',\n",
    "#  'INC_AVG_LTD',\n",
    "#  'BJP_AVG_LTD',\n",
    "#  'NCP_AVG_LTD',\n",
    "#  'CPI_AVG_LTD',\n",
    "#  'CPI(M)_AVG_LTD',\n",
    "#  'INC_MAX_LTD',\n",
    "#  'BJP_MAX_LTD',\n",
    "#  'NCP_MAX_LTD',\n",
    "#  'CPI_MAX_LTD',\n",
    "#  'CPI(M)_MAX_LTD'\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "party_list=df.select(F.collect_set('Party').alias('Party')).first()['Party']\n",
    "aggregate_fn=[\"SUM\",\"COUNT\",\"AVG\",\"MAX\"]\n",
    "for party_name in party_list:\n",
    "    summ=df.withColumn(\"Amount\",df.Amount).groupBy(\"Party\").sum(\"Amount\").filter(col(\"Party\").like(party_name)).collect()[0][1]\n",
    "    counts=df.withColumn(\"Amount\",df.Amount).groupBy(\"Party\").count().filter(col(\"Party\").like(party_name)).collect()[0][1]\n",
    "    average=df.withColumn(\"Amount\",df.Amount).groupBy(\"Party\").avg(\"Amount\").filter(col(\"Party\").like(party_name)).collect()[0][1]\n",
    "    maximum=df.withColumn(\"Amount\",df.Amount).groupBy(\"Party\").max(\"Amount\").filter(col(\"Party\").like(party_name)).collect()[0][1]\n",
    "\n",
    "    for ag_fn in aggregate_fn:\n",
    "        if ag_fn==\"SUM\":\n",
    "            df=df.withColumn(party_name+\"_\"+ag_fn+\"_LTD\",when(df.Party.contains(party_name),summ).otherwise(0))\n",
    "        elif ag_fn == \"COUNT\":\n",
    "            df=df.withColumn(party_name+\"_\"+ag_fn+\"_LTD\",when(df.Party.contains(party_name),counts).otherwise(0))\n",
    "        elif ag_fn == \"AVG\":\n",
    "           df= df.withColumn(party_name+\"_\"+ag_fn+\"_LTD\",when(df.Party.contains(party_name),average).otherwise(0))\n",
    "        elif ag_fn == \"MAX\":\n",
    "           df=df.withColumn(party_name+\"_\"+ag_fn+\"_LTD\",when(df.Party.contains(party_name),maximum).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tCalculate top donor per party \n",
    "\n",
    "# o\tBJP_TOP_DONOR\n",
    "# o\tCPI_TOP_DONOR\n",
    "# o\tINC_TOP_DONOR\n",
    "# o\tNCP_TOP_DONOR\n",
    "# o\tCPI(M)_TOP_DONOR\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"Party\").orderBy(col(\"Amount\").desc())\n",
    "df_top=df.withColumn(\"row_number\",row_number().over(windowSpec)).filter(col(\"row_number\").like(\"1\"))\n",
    "\n",
    "for rows in df_top.collect():\n",
    "    df=df.withColumn(rows[\"Party\"]+\"_TOP_DONOR\",when(df.Party.contains(rows[\"Party\"]),rows[\"Name\"]).otherwise(\"-\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tCalculate sum of donations per financial year per party and the column should be named as 2011-12_BJP_SUM\n",
    "gbo=df.groupBy(\"Party\",\"fin_year\").sum(\"Amount\").orderBy(\"fin_year\")\n",
    "for rows in gbo.collect():\n",
    "    df=df.withColumn(rows[\"fin_year\"]+\"_\"+rows[\"Party\"]+\"_SUM\",when(df.Party.contains(rows[\"Party\"]) & df.fin_year.contains(rows[\"fin_year\"]),rows[\"sum(Amount)\"]).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# •\tCalculate number of donation till date per mode_of_payment\n",
    "# o\tBank_count_LTD\n",
    "# o\tCheque_count_LTD\n",
    "# o\tCash_count_LTD\n",
    "# o\tOthers_count_LTD\n",
    "\n",
    "dfOne= df.groupBy(\"mode_of_payment\").count()\n",
    "for rows in dfOne.collect():\n",
    "    df=df.withColumn(rows[\"mode_of_payment\"]+\"_count_LTD\",when(df.mode_of_payment.contains(rows[\"mode_of_payment\"]),rows[\"count\"]).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"pyspark_assignment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbf1c7f8b01ccb84dc85d2bd9a0ab6bc50d1a2dc5b428ef4a28ab39c41d20a72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
